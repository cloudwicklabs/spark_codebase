package com.cloudwick.spark.loganalysis

import java.io.{FileNotFoundException, File}
import java.net.InetAddress

import com.maxmind.geoip2.DatabaseReader.Builder
import com.maxmind.geoip2.exception.AddressNotFoundException
import com.maxmind.geoip2.model.CityResponse
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.Time
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.{Logging, SparkFiles}
import org.joda.time.format.DateTimeFormat

import scala.concurrent.duration._

/**
 * Log analytics use-case for Apache log-events generated by
 * [[https://github.com/cloudwicklabs/generator CloudwickLabs Generator]]
 *
 * @author ashrith
 */
object LogAnalyzer extends Logging {

  type StatusHandler = (RDD[StatusCount], Time) => Unit
  type VolumeHandler = (RDD[VolumeCount], Time) => Unit
  type CountryHandler = (RDD[CountryCount], Time) => Unit

  private val logEventPattern = """([\d.]+) (\S+) (\S+) \[(.*)\] "([^\s]+) (/[^\s]*) HTTP/[^\s]+" (\d{3}) (\d+) "([^"]+)" "([^"]+)"""".r
  private val formatter = DateTimeFormat.forPattern("dd/MMM/yyyy:HH:mm:ss Z")

  if (!new File(SparkFiles.getRootDirectory(), "GeoLite2-City.mmdb").exists()) {
    throw new FileNotFoundException("Please pass GeoLite2-City.mmdb using --files from spark-submit")
  }
  private val dbFile = new File(SparkFiles.get("GeoLite2-City.mmdb"))
  private val dbReader = new Builder(dbFile).build()

  def parseLogEvent(le: String): Option[LogEvent] = {
    le match {
      case logEventPattern(ip, ci, ui, ts, rt, rp, rc, rs, r, ua) =>
        Some(LogEvent(ip, ci, ui, formatter.parseDateTime(ts), rt, rp, rc.toInt, rs.toInt, r, ua))
      case _ => None
    }
  }

  def prepareEvents(lines: RDD[String]): RDD[LogEvent] = {
    // NOTE: flatMap removes the None values generated by parseLogEvent
    lines.flatMap(_.split("\\n")).flatMap(parseLogEvent)
  }

  def resolveIp(ip: String): Option[Location] = {
    val ipAddress = InetAddress.getByName(ip)
    try {
      val response = dbReader.city(ipAddress)
      Some(
        Location(ip,
          response.getCountry.getIsoCode,
          response.getCity.getName,
          response.getLocation.getLatitude,
          response.getLocation.getLongitude))
    } catch {
      case ex: AddressNotFoundException => None
    }

  }

  def statusCounter(lines: DStream[String])(handler: StatusHandler): Unit = {
    val events = lines.transform(prepareEvents _)

    val statusCounts = events.map(event => (event.responseCode, 1L)).reduceByKey(_ + _).map {
      case (statusCode: Int, count: Long) => StatusCount(statusCode, count)
    }

    statusCounts.foreachRDD((rdd: RDD[StatusCount], time: Time) => {
      handler(rdd.sortBy(_.status), time)
    })
  }

  def volumeCounter(lines: DStream[String])(handler: VolumeHandler): Unit = {
    val events = lines.transform(prepareEvents _)

    val volumeCounts = events.map { event =>
      val millis = event.timeStamp.getMillis
      val minutes = Duration(millis, MILLISECONDS).toMinutes
      (minutes, 1L)
    }.reduceByKey(_ + _).map {
      case (minute: Long, count: Long) => VolumeCount(minute, count)
    }

    volumeCounts.foreachRDD((rdd: RDD[VolumeCount], time: Time) => {
      handler(rdd.sortBy(_.timeStamp), time)
    })
  }

  def countryCounter(lines: DStream[String])(handler: CountryHandler): Unit = {
    val events = lines.transform(prepareEvents _)

    val countryCounts = events.flatMap(
      event => resolveIp(event.ip)
    ).map(loc => (loc.country, 1L)).reduceByKey(_ + _).map {
      case(country: String, count: Long) => CountryCount(country, count)
    }

    countryCounts.foreachRDD((rdd: RDD[CountryCount], time: Time) => {
      handler(rdd.sortBy(_.country), time)
    })
  }
}
