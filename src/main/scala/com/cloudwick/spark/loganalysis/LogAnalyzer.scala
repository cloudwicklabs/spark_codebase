package com.cloudwick.spark.loganalysis

import java.io.{File, FileNotFoundException}
import java.net.InetAddress

import com.cloudwick.cassandra.{CassandraLocationVisitServiceModule, CassandraLogVolumeServiceModule, CassandraStatusCountServiceModule}
import com.cloudwick.cassandra.schema.{LocationVisit, LogVolume, StatusCount}
import com.maxmind.geoip2.DatabaseReader.Builder
import com.maxmind.geoip2.exception.AddressNotFoundException
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.Time
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.{Logging, SparkFiles}
import org.joda.time.format.DateTimeFormat

import scala.concurrent.duration._

/**
 * Log analytics use-case for Apache log-events generated by
 * [[https://github.com/cloudwicklabs/generator CloudwickLabs Generator]]
 *
 * @author ashrith
 */
object LogAnalyzer extends CassandraStatusCountServiceModule
  with CassandraLogVolumeServiceModule with CassandraLocationVisitServiceModule with Logging {

  type StatusHandler = (RDD[StatusCount], Time) => Unit
  type VolumeHandler = (RDD[LogVolume], Time) => Unit
  type LocationHandler = (RDD[LocationVisit], Time) => Unit

  private val logEventPattern = """([\d.]+) (\S+) (\S+) \[(.*)\] "([^\s]+) (/[^\s]*) HTTP/[^\s]+" (\d{3}) (\d+) "([^"]+)" "([^"]+)"""".r
  private val formatter = DateTimeFormat.forPattern("dd/MMM/yyyy:HH:mm:ss Z")

  if (!new File(SparkFiles.getRootDirectory(), "GeoLite2-City.mmdb").exists()) {
    throw new FileNotFoundException("Please pass GeoLite2-City.mmdb using --files from spark-submit")
  }
  private val dbFile = new File(SparkFiles.get("GeoLite2-City.mmdb"))
  private val dbReader = new Builder(dbFile).build()

  def parseLogEvent(le: String): Option[LogEvent] = {
    le match {
      case logEventPattern(ip, ci, ui, ts, rt, rp, rc, rs, r, ua) =>
        Some(LogEvent(ip, ci, ui, formatter.parseDateTime(ts), rt, rp, rc.toInt, rs.toInt, r, ua))
      case _ => None
    }
  }

  def prepareEvents(lines: RDD[String]): RDD[LogEvent] = {
    // NOTE: flatMap removes the None values generated by parseLogEvent
    lines.flatMap(_.split("\\n")).flatMap(parseLogEvent)
  }

  def resolveIp(ip: String): Option[Location] = {
    val ipAddress = InetAddress.getByName(ip)
    try {
      val response = dbReader.city(ipAddress)
      val country  = response.getCountry.getIsoCode match { case ""|null => "US" case x => x }
      val city = response.getCity.getName match { case ""|null => "<empty>" case x => x }
      Some(
        Location(ip,
          country,
          city,
          response.getLocation.getLatitude,
          response.getLocation.getLongitude))
    } catch {
      case ex: AddressNotFoundException => None
    }

  }

  def statusCounter(lines: DStream[String])(handler: StatusHandler): Unit = {
    val events = lines.transform(prepareEvents _)

    val statusCounts = events.map(event => (event.responseCode, 1L)).reduceByKey(_ + _).map {
      case (statusCode: Int, count: Long) => StatusCount(statusCode, count)
    }

    statusCounts.foreachRDD((rdd: RDD[StatusCount], time: Time) => {
      handler(rdd.sortBy(_.statusCode), time) // executed at the driver

      rdd.foreachPartition(partitionRecords => {
        partitionRecords.foreach(statusCountService.update)
      })
    })
  }

  def volumeCounter(lines: DStream[String])(handler: VolumeHandler): Unit = {
    val events = lines.transform(prepareEvents _)

    val volumeCounts = events.map { event =>
      val millis = event.timeStamp.getMillis
      val minutes = Duration(millis, MILLISECONDS).toMinutes
      (minutes, 1L)
    }.reduceByKey(_ + _).map {
      case (minute: Long, count: Long) => LogVolume(minute, count)
    }

    volumeCounts.foreachRDD((rdd: RDD[LogVolume], time: Time) => {
      handler(rdd.sortBy(_.timeStamp), time)

      rdd.foreachPartition(partitionRecords => {
        partitionRecords.foreach(logVolumeService.update)
      })
    })
  }

  def countryCounter(lines: DStream[String])(handler: LocationHandler): Unit = {
    val events = lines.transform(prepareEvents _)

    val countryCounts = events.flatMap(
      event => resolveIp(event.ip)
    ).map(loc => ((loc.country, loc.city), 1L)).reduceByKey(_ + _).map {
      case((country: String, city: String), count: Long) => LocationVisit(country, city, count)
    }

    countryCounts.foreachRDD((rdd: RDD[LocationVisit], time: Time) => {
      handler(rdd.sortBy(_.country), time)

      rdd.foreachPartition(partitionRecords => {
        partitionRecords.foreach(locationVisitService.update)
      })
    })
  }
}
